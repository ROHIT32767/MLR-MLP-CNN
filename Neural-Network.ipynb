{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCDECMIEokMX"
      },
      "source": [
        "```\n",
        "Input: (Hours Slept, Hours Studied)\n",
        "Output: (Test Score)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zh8uuf6dokMe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import optimize\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o42_ALbJokMh"
      },
      "outputs": [],
      "source": [
        "X = np.array(([3,5], [5,1], [10,2]), dtype=float)\n",
        "y = np.array(([75], [82], [93]), dtype=float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7oQkh8iokMi"
      },
      "source": [
        "### Scaling data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Phc4EtZHokMi"
      },
      "outputs": [],
      "source": [
        "X = X / np.amax(X, axis=0)\n",
        "y = y / 100 # Max test score is 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CmB-EwyFokMk"
      },
      "outputs": [],
      "source": [
        "class Neural_Network(object):\n",
        "    def __init__(self, inputLayerSize: int = 2, hiddenLayerSize: int = 3, outputLayerSize: int = 1):\n",
        "        # Define Hyperparameters\n",
        "        self.inputLayerSize = inputLayerSize\n",
        "        self.outputLayerSize = outputLayerSize\n",
        "        self.hiddenLayerSize = hiddenLayerSize\n",
        "\n",
        "        # Weights (parameters)\n",
        "        self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
        "        self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
        "\n",
        "    def forward(self, X):\n",
        "        '''\n",
        "        Propagate inputs though network\n",
        "        Returns a new array with the same shape as target array y.\n",
        "\n",
        "        z2 = X.W1 where X is input matrix and W1 is weight matrix from input to hidden layer\n",
        "        a2 = f(z2) where f is sigmoid activation function\n",
        "        z3 = a2.W2 where a2 is hidden layer output matrix and W2 is weight matrix from hidden to output layer\n",
        "        yHat = f(z3) where f is sigmoid activation function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray\n",
        "            Input data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        out : ndarray\n",
        "            Predicted data.\n",
        "        '''\n",
        "        self.z2 = np.dot(X, self.W1)        # dot product of X (input) and first set of 2x3 weights\n",
        "        self.a2 = self.sigmoid(self.z2)     # apply sigmoid activation function to z2\n",
        "        self.z3 = np.dot(self.a2, self.W2)  # dot product of hidden layer (a2) and second set of 3x1 weights\n",
        "        yHat = self.sigmoid(self.z3)        # apply sigmoid activation function to z3\n",
        "        return yHat\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        '''\n",
        "        Apply sigmoid activation function to scalar, vector, or matrix\n",
        "        '''\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def sigmoidPrime(self, z):\n",
        "        '''\n",
        "        Derivative of sigmoid function\n",
        "        Returns the derivative of the sigmoid function evaluated at z\n",
        "        '''\n",
        "        return np.exp(-z)/((1 + np.exp(-z))**2)\n",
        "\n",
        "    def costFunction(self, X, y):\n",
        "        \"\"\"\n",
        "        Computes cost for given X,y using weights already stored in class.\n",
        "        Returns a new array with the same shape as y.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray\n",
        "            Input data.\n",
        "        y : ndarray\n",
        "            Target data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        out : ndarray\n",
        "            Cost of the predicted data with respect to true target data.\n",
        "\n",
        "        \"\"\"\n",
        "        self.yHat = self.forward(X)\n",
        "        J = 0.5* sum((y-self.yHat)**2)\n",
        "        return J\n",
        "\n",
        "    def costFunctionPrime(self, X, y):\n",
        "        \"\"\"\n",
        "        Computes derivative with respect to W1 and W2 for a given X and y\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray\n",
        "            Input data.\n",
        "        y : ndarray\n",
        "            Target data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        out : Tuple[ndarray, ndarray]\n",
        "            Derivative of cost function with respect to W1 and W2 respectively.\n",
        "        \"\"\"\n",
        "        self.yHat = self.forward(X)\n",
        "\n",
        "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
        "        dJdW2 = np.dot(self.a2.T, delta3)\n",
        "\n",
        "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
        "        dJdW1 = np.dot(X.T, delta2)\n",
        "\n",
        "        return dJdW1, dJdW2\n",
        "\n",
        "    #Helper Functions for interacting with other classes:\n",
        "    def getParams(self):\n",
        "        #Get W1 and W2 unrolled into vector:\n",
        "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
        "        return params\n",
        "\n",
        "    def setParams(self, params):\n",
        "        #Set W1 and W2 using single paramater vector.\n",
        "        W1_start = 0\n",
        "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
        "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
        "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
        "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
        "\n",
        "    def computeGradients(self, X, y):\n",
        "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
        "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uZEcFozxokMo"
      },
      "outputs": [],
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, N):\n",
        "        # Make Local reference to network:\n",
        "        self.N = N\n",
        "\n",
        "    def train(self, X, y):\n",
        "        #Make an internal variable for the callback function:\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        #Make empty list to store costs:\n",
        "        self.J = []\n",
        "\n",
        "        loss = self.N.costFunction(X, y)\n",
        "        k = 0\n",
        "        maxiter =  300000\n",
        "        lr = 0.06\n",
        "\n",
        "        while (loss > 1e-5) and (k < maxiter):\n",
        "            grad = self.N.computeGradients(X, y)\n",
        "            nW1s = self.N.inputLayerSize * self.N.hiddenLayerSize\n",
        "            dJW1 = grad[:nW1s].reshape(self.N.inputLayerSize, self.N.hiddenLayerSize)\n",
        "            dJW2 = grad[nW1s:].reshape(self.N.hiddenLayerSize, self.N.outputLayerSize)\n",
        "            self.N.W1 -= lr * dJW1\n",
        "            self.N.W2 -= lr * dJW2\n",
        "            loss = self.N.costFunction(X, y)\n",
        "            self.J.append(loss)\n",
        "            k += 1\n",
        "            if k % 1000 == 0:\n",
        "                print(\"Iteration %d: loss = %f\" % (k, loss[0]))\n",
        "\n",
        "        print(\"Operation terminated successfully.\")\n",
        "        print(\"         Iterations: %d\" % k)\n",
        "        print(\"         Final loss function: %f\" % loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLGiMHFFokMq",
        "outputId": "e0bd2411-31d2-4932-a543-66490e330239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1000: loss = 0.005854\n",
            "Iteration 2000: loss = 0.003374\n",
            "Iteration 3000: loss = 0.002033\n",
            "Iteration 4000: loss = 0.001353\n",
            "Iteration 5000: loss = 0.001015\n",
            "Iteration 6000: loss = 0.000843\n",
            "Iteration 7000: loss = 0.000748\n",
            "Iteration 8000: loss = 0.000689\n",
            "Iteration 9000: loss = 0.000647\n",
            "Iteration 10000: loss = 0.000614\n",
            "Iteration 11000: loss = 0.000586\n",
            "Iteration 12000: loss = 0.000560\n",
            "Iteration 13000: loss = 0.000537\n",
            "Iteration 14000: loss = 0.000516\n",
            "Iteration 15000: loss = 0.000496\n",
            "Iteration 16000: loss = 0.000477\n",
            "Iteration 17000: loss = 0.000460\n",
            "Iteration 18000: loss = 0.000443\n",
            "Iteration 19000: loss = 0.000428\n",
            "Iteration 20000: loss = 0.000413\n",
            "Iteration 21000: loss = 0.000399\n",
            "Iteration 22000: loss = 0.000385\n",
            "Iteration 23000: loss = 0.000373\n",
            "Iteration 24000: loss = 0.000361\n",
            "Iteration 25000: loss = 0.000349\n",
            "Iteration 26000: loss = 0.000339\n",
            "Iteration 27000: loss = 0.000328\n",
            "Iteration 28000: loss = 0.000318\n",
            "Iteration 29000: loss = 0.000309\n",
            "Iteration 30000: loss = 0.000300\n",
            "Iteration 31000: loss = 0.000291\n",
            "Iteration 32000: loss = 0.000282\n",
            "Iteration 33000: loss = 0.000274\n",
            "Iteration 34000: loss = 0.000267\n",
            "Iteration 35000: loss = 0.000259\n",
            "Iteration 36000: loss = 0.000252\n",
            "Iteration 37000: loss = 0.000245\n",
            "Iteration 38000: loss = 0.000239\n",
            "Iteration 39000: loss = 0.000232\n",
            "Iteration 40000: loss = 0.000226\n",
            "Iteration 41000: loss = 0.000220\n",
            "Iteration 42000: loss = 0.000214\n",
            "Iteration 43000: loss = 0.000209\n",
            "Iteration 44000: loss = 0.000204\n",
            "Iteration 45000: loss = 0.000198\n",
            "Iteration 46000: loss = 0.000193\n",
            "Iteration 47000: loss = 0.000188\n",
            "Iteration 48000: loss = 0.000184\n",
            "Iteration 49000: loss = 0.000179\n",
            "Iteration 50000: loss = 0.000175\n",
            "Iteration 51000: loss = 0.000171\n",
            "Iteration 52000: loss = 0.000166\n",
            "Iteration 53000: loss = 0.000162\n",
            "Iteration 54000: loss = 0.000159\n",
            "Iteration 55000: loss = 0.000155\n",
            "Iteration 56000: loss = 0.000151\n",
            "Iteration 57000: loss = 0.000147\n",
            "Iteration 58000: loss = 0.000144\n",
            "Iteration 59000: loss = 0.000141\n",
            "Iteration 60000: loss = 0.000137\n",
            "Iteration 61000: loss = 0.000134\n",
            "Iteration 62000: loss = 0.000131\n",
            "Iteration 63000: loss = 0.000128\n",
            "Iteration 64000: loss = 0.000125\n",
            "Iteration 65000: loss = 0.000122\n",
            "Iteration 66000: loss = 0.000120\n",
            "Iteration 67000: loss = 0.000117\n",
            "Iteration 68000: loss = 0.000114\n",
            "Iteration 69000: loss = 0.000112\n",
            "Iteration 70000: loss = 0.000109\n",
            "Iteration 71000: loss = 0.000107\n",
            "Iteration 72000: loss = 0.000105\n",
            "Iteration 73000: loss = 0.000102\n",
            "Iteration 74000: loss = 0.000100\n",
            "Iteration 75000: loss = 0.000098\n",
            "Iteration 76000: loss = 0.000096\n",
            "Iteration 77000: loss = 0.000094\n",
            "Iteration 78000: loss = 0.000092\n",
            "Iteration 79000: loss = 0.000090\n",
            "Iteration 80000: loss = 0.000088\n",
            "Iteration 81000: loss = 0.000086\n",
            "Iteration 82000: loss = 0.000084\n",
            "Iteration 83000: loss = 0.000082\n",
            "Iteration 84000: loss = 0.000080\n",
            "Iteration 85000: loss = 0.000079\n",
            "Iteration 86000: loss = 0.000077\n",
            "Iteration 87000: loss = 0.000076\n",
            "Iteration 88000: loss = 0.000074\n",
            "Iteration 89000: loss = 0.000072\n",
            "Iteration 90000: loss = 0.000071\n",
            "Iteration 91000: loss = 0.000069\n",
            "Iteration 92000: loss = 0.000068\n",
            "Iteration 93000: loss = 0.000067\n",
            "Iteration 94000: loss = 0.000065\n",
            "Iteration 95000: loss = 0.000064\n",
            "Iteration 96000: loss = 0.000063\n",
            "Iteration 97000: loss = 0.000061\n",
            "Iteration 98000: loss = 0.000060\n",
            "Iteration 99000: loss = 0.000059\n",
            "Iteration 100000: loss = 0.000058\n",
            "Iteration 101000: loss = 0.000056\n",
            "Iteration 102000: loss = 0.000055\n",
            "Iteration 103000: loss = 0.000054\n",
            "Iteration 104000: loss = 0.000053\n",
            "Iteration 105000: loss = 0.000052\n",
            "Iteration 106000: loss = 0.000051\n",
            "Iteration 107000: loss = 0.000050\n",
            "Iteration 108000: loss = 0.000049\n",
            "Iteration 109000: loss = 0.000048\n",
            "Iteration 110000: loss = 0.000047\n",
            "Iteration 111000: loss = 0.000046\n",
            "Iteration 112000: loss = 0.000045\n",
            "Iteration 113000: loss = 0.000044\n",
            "Iteration 114000: loss = 0.000044\n",
            "Iteration 115000: loss = 0.000043\n",
            "Iteration 116000: loss = 0.000042\n",
            "Iteration 117000: loss = 0.000041\n",
            "Iteration 118000: loss = 0.000040\n",
            "Iteration 119000: loss = 0.000039\n",
            "Iteration 120000: loss = 0.000039\n",
            "Iteration 121000: loss = 0.000038\n",
            "Iteration 122000: loss = 0.000037\n",
            "Iteration 123000: loss = 0.000036\n",
            "Iteration 124000: loss = 0.000036\n",
            "Iteration 125000: loss = 0.000035\n",
            "Iteration 126000: loss = 0.000034\n",
            "Iteration 127000: loss = 0.000034\n",
            "Iteration 128000: loss = 0.000033\n",
            "Iteration 129000: loss = 0.000032\n",
            "Iteration 130000: loss = 0.000032\n",
            "Iteration 131000: loss = 0.000031\n",
            "Iteration 132000: loss = 0.000031\n",
            "Iteration 133000: loss = 0.000030\n",
            "Iteration 134000: loss = 0.000029\n",
            "Iteration 135000: loss = 0.000029\n",
            "Iteration 136000: loss = 0.000028\n",
            "Iteration 137000: loss = 0.000028\n",
            "Iteration 138000: loss = 0.000027\n",
            "Iteration 139000: loss = 0.000027\n",
            "Iteration 140000: loss = 0.000026\n",
            "Iteration 141000: loss = 0.000026\n",
            "Iteration 142000: loss = 0.000025\n",
            "Iteration 143000: loss = 0.000025\n",
            "Iteration 144000: loss = 0.000024\n",
            "Iteration 145000: loss = 0.000024\n",
            "Iteration 146000: loss = 0.000023\n",
            "Iteration 147000: loss = 0.000023\n",
            "Iteration 148000: loss = 0.000023\n",
            "Iteration 149000: loss = 0.000022\n",
            "Iteration 150000: loss = 0.000022\n",
            "Iteration 151000: loss = 0.000021\n",
            "Iteration 152000: loss = 0.000021\n",
            "Iteration 153000: loss = 0.000021\n",
            "Iteration 154000: loss = 0.000020\n",
            "Iteration 155000: loss = 0.000020\n",
            "Iteration 156000: loss = 0.000019\n",
            "Iteration 157000: loss = 0.000019\n",
            "Iteration 158000: loss = 0.000019\n",
            "Iteration 159000: loss = 0.000018\n",
            "Iteration 160000: loss = 0.000018\n",
            "Iteration 161000: loss = 0.000018\n",
            "Iteration 162000: loss = 0.000017\n",
            "Iteration 163000: loss = 0.000017\n",
            "Iteration 164000: loss = 0.000017\n",
            "Iteration 165000: loss = 0.000016\n",
            "Iteration 166000: loss = 0.000016\n",
            "Iteration 167000: loss = 0.000016\n",
            "Iteration 168000: loss = 0.000016\n",
            "Iteration 169000: loss = 0.000015\n",
            "Iteration 170000: loss = 0.000015\n",
            "Iteration 171000: loss = 0.000015\n",
            "Iteration 172000: loss = 0.000015\n",
            "Iteration 173000: loss = 0.000014\n",
            "Iteration 174000: loss = 0.000014\n",
            "Iteration 175000: loss = 0.000014\n",
            "Iteration 176000: loss = 0.000013\n",
            "Iteration 177000: loss = 0.000013\n",
            "Iteration 178000: loss = 0.000013\n",
            "Iteration 179000: loss = 0.000013\n",
            "Iteration 180000: loss = 0.000013\n",
            "Iteration 181000: loss = 0.000012\n",
            "Iteration 182000: loss = 0.000012\n",
            "Iteration 183000: loss = 0.000012\n",
            "Iteration 184000: loss = 0.000012\n",
            "Iteration 185000: loss = 0.000011\n",
            "Iteration 186000: loss = 0.000011\n",
            "Iteration 187000: loss = 0.000011\n",
            "Iteration 188000: loss = 0.000011\n",
            "Iteration 189000: loss = 0.000011\n",
            "Iteration 190000: loss = 0.000010\n",
            "Iteration 191000: loss = 0.000010\n",
            "Iteration 192000: loss = 0.000010\n",
            "Operation terminated successfully.\n",
            "         Iterations: 192549\n",
            "         Final loss function: 0.000010\n"
          ]
        }
      ],
      "source": [
        "NN = Neural_Network()\n",
        "T = Trainer(NN)\n",
        "T.train(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stVg-EHWokMs"
      },
      "source": [
        "### Predicted Train values vs Actual Train values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcD-NUn2okMt",
        "outputId": "746095e5-1398-47f5-bc42-c4d1c7978a9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[0.74998366],\n",
              "        [0.82211746],\n",
              "        [0.92606096]]),\n",
              " array([[0.75],\n",
              "        [0.82],\n",
              "        [0.93]]))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "NN.forward(X), y"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dip",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
