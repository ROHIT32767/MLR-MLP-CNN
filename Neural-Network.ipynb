{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCDECMIEokMX"
      },
      "source": [
        "```\n",
        "Input: (Hours Slept, Hours Studied)\n",
        "Output: (Test Score)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Zh8uuf6dokMe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import optimize\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "o42_ALbJokMh"
      },
      "outputs": [],
      "source": [
        "X = np.array(([3,5], [5,1], [10,2]), dtype=float)\n",
        "y = np.array(([75], [82], [93]), dtype=float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7oQkh8iokMi"
      },
      "source": [
        "### Scaling data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Phc4EtZHokMi"
      },
      "outputs": [],
      "source": [
        "X = X / np.amax(X, axis=0)\n",
        "y = y / 100 # Max test score is 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "CmB-EwyFokMk"
      },
      "outputs": [],
      "source": [
        "class Neural_Network(object):\n",
        "    def __init__(self, inputLayerSize: int = 2, hiddenLayerSize: int = 3, outputLayerSize: int = 1):\n",
        "        # Define Hyperparameters\n",
        "        self.inputLayerSize = inputLayerSize\n",
        "        self.outputLayerSize = outputLayerSize\n",
        "        self.hiddenLayerSize = hiddenLayerSize\n",
        "\n",
        "        # Weights (parameters)\n",
        "        np.random.seed(42)\n",
        "        self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
        "        self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
        "        # print('weights')\n",
        "        # print(self.W1)\n",
        "        # print(self.W2)\n",
        "        # print('-----------')\n",
        "\n",
        "    def forward(self, X):\n",
        "        '''\n",
        "        Propagate inputs though network\n",
        "        Returns a new array with the same shape as target array y.\n",
        "\n",
        "        z2 = X.W1 where X is input matrix and W1 is weight matrix from input to hidden layer\n",
        "        a2 = f(z2) where f is sigmoid activation function\n",
        "        z3 = a2.W2 where a2 is hidden layer output matrix and W2 is weight matrix from hidden to output layer\n",
        "        yHat = f(z3) where f is sigmoid activation function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray\n",
        "            Input data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        out : ndarray\n",
        "            Predicted data.\n",
        "        '''\n",
        "        self.z2 = np.dot(X, self.W1)        # dot product of X (input) and first set of 2x3 weights\n",
        "        self.a2 = self.sigmoid(self.z2)     # apply sigmoid activation function to z2\n",
        "        self.z3 = np.dot(self.a2, self.W2)  # dot product of hidden layer (a2) and second set of 3x1 weights\n",
        "        yHat = self.sigmoid(self.z3)        # apply sigmoid activation function to z3\n",
        "        # print('Activations')\n",
        "        # print(X,self.a2,yHat)\n",
        "        # print('-----------')\n",
        "        # print('weighted_sums')\n",
        "        # print(self.z2,self.z3)\n",
        "        # print('------------')\n",
        "        return yHat\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        '''\n",
        "        Apply sigmoid activation function to scalar, vector, or matrix\n",
        "        '''\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def sigmoidPrime(self, z):\n",
        "        '''\n",
        "        Derivative of sigmoid function\n",
        "        Returns the derivative of the sigmoid function evaluated at z\n",
        "        '''\n",
        "        return np.exp(-z)/((1 + np.exp(-z))**2)\n",
        "\n",
        "    def costFunction(self, X, y):\n",
        "        \"\"\"\n",
        "        Computes cost for given X,y using weights already stored in class.\n",
        "        Returns a new array with the same shape as y.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray\n",
        "            Input data.\n",
        "        y : ndarray\n",
        "            Target data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        out : ndarray\n",
        "            Cost of the predicted data with respect to true target data.\n",
        "\n",
        "        \"\"\"\n",
        "        self.yHat = self.forward(X)\n",
        "        J = 0.5* sum((y-self.yHat)**2)\n",
        "        return J\n",
        "\n",
        "    def costFunctionPrime(self, X, y):\n",
        "        \"\"\"\n",
        "        Computes derivative with respect to W1 and W2 for a given X and y\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray\n",
        "            Input data.\n",
        "        y : ndarray\n",
        "            Target data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        out : Tuple[ndarray, ndarray]\n",
        "            Derivative of cost function with respect to W1 and W2 respectively.\n",
        "        \"\"\"\n",
        "        self.yHat = self.forward(X)\n",
        "        # print(self.yHat)\n",
        "\n",
        "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
        "        dJdW2 = np.dot(self.a2.T, delta3)\n",
        "\n",
        "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
        "        dJdW1 = np.dot(X.T, delta2)\n",
        "        # print('deltas',delta2,delta3)\n",
        "\n",
        "        return dJdW1, dJdW2\n",
        "\n",
        "    #Helper Functions for interacting with other classes:\n",
        "    def getParams(self):\n",
        "        #Get W1 and W2 unrolled into vector:\n",
        "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
        "        return params\n",
        "\n",
        "    def setParams(self, params):\n",
        "        #Set W1 and W2 using single paramater vector.\n",
        "        W1_start = 0\n",
        "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
        "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
        "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
        "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
        "\n",
        "    def computeGradients(self, X, y):\n",
        "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
        "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "uZEcFozxokMo"
      },
      "outputs": [],
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, N):\n",
        "        # Make Local reference to network:\n",
        "        self.N = N\n",
        "\n",
        "    def train(self, X, y):\n",
        "        #Make an internal variable for the callback function:\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        #Make empty list to store costs:\n",
        "        self.J = []\n",
        "\n",
        "        loss = self.N.costFunction(X, y)\n",
        "        k = 0\n",
        "        maxiter =  300000\n",
        "        # maxiter = 1\n",
        "        lr = 0.06\n",
        "\n",
        "        while (loss > 1e-5) and (k < maxiter):\n",
        "            grad = self.N.computeGradients(X, y)\n",
        "            # print('Gradients')\n",
        "            # print(grad)\n",
        "            # print('--------------')\n",
        "            nW1s = self.N.inputLayerSize * self.N.hiddenLayerSize\n",
        "            dJW1 = grad[:nW1s].reshape(self.N.inputLayerSize, self.N.hiddenLayerSize)\n",
        "            dJW2 = grad[nW1s:].reshape(self.N.hiddenLayerSize, self.N.outputLayerSize)\n",
        "            self.N.W1 -= lr * dJW1\n",
        "            self.N.W2 -= lr * dJW2\n",
        "            loss = self.N.costFunction(X, y)\n",
        "            self.J.append(loss)\n",
        "            k += 1\n",
        "            if k % 1000 == 0:\n",
        "                print(\"Iteration %d: loss = %f\" % (k, loss[0]))\n",
        "\n",
        "        print(\"Operation terminated successfully.\")\n",
        "        print(\"         Iterations: %d\" % k)\n",
        "        print(\"         Final loss function: %f\" % loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLGiMHFFokMq",
        "outputId": "e0bd2411-31d2-4932-a543-66490e330239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1000: loss = 0.009045\n",
            "Iteration 2000: loss = 0.007988\n",
            "Iteration 3000: loss = 0.007083\n",
            "Iteration 4000: loss = 0.006272\n",
            "Iteration 5000: loss = 0.005526\n",
            "Iteration 6000: loss = 0.004837\n",
            "Iteration 7000: loss = 0.004206\n",
            "Iteration 8000: loss = 0.003642\n",
            "Iteration 9000: loss = 0.003152\n",
            "Iteration 10000: loss = 0.002740\n",
            "Iteration 11000: loss = 0.002404\n",
            "Iteration 12000: loss = 0.002138\n",
            "Iteration 13000: loss = 0.001932\n",
            "Iteration 14000: loss = 0.001777\n",
            "Iteration 15000: loss = 0.001660\n",
            "Iteration 16000: loss = 0.001573\n",
            "Iteration 17000: loss = 0.001508\n",
            "Iteration 18000: loss = 0.001459\n",
            "Iteration 19000: loss = 0.001421\n",
            "Iteration 20000: loss = 0.001391\n",
            "Iteration 21000: loss = 0.001367\n",
            "Iteration 22000: loss = 0.001347\n",
            "Iteration 23000: loss = 0.001330\n",
            "Iteration 24000: loss = 0.001314\n",
            "Iteration 25000: loss = 0.001301\n",
            "Iteration 26000: loss = 0.001288\n",
            "Iteration 27000: loss = 0.001277\n",
            "Iteration 28000: loss = 0.001266\n",
            "Iteration 29000: loss = 0.001256\n",
            "Iteration 30000: loss = 0.001246\n",
            "Iteration 31000: loss = 0.001237\n",
            "Iteration 32000: loss = 0.001228\n",
            "Iteration 33000: loss = 0.001220\n",
            "Iteration 34000: loss = 0.001212\n",
            "Iteration 35000: loss = 0.001204\n",
            "Iteration 36000: loss = 0.001196\n",
            "Iteration 37000: loss = 0.001188\n",
            "Iteration 38000: loss = 0.001181\n",
            "Iteration 39000: loss = 0.001174\n",
            "Iteration 40000: loss = 0.001166\n",
            "Iteration 41000: loss = 0.001159\n",
            "Iteration 42000: loss = 0.001152\n",
            "Iteration 43000: loss = 0.001145\n",
            "Iteration 44000: loss = 0.001138\n",
            "Iteration 45000: loss = 0.001131\n",
            "Iteration 46000: loss = 0.001124\n",
            "Iteration 47000: loss = 0.001117\n",
            "Iteration 48000: loss = 0.001109\n",
            "Iteration 49000: loss = 0.001102\n",
            "Iteration 50000: loss = 0.001095\n",
            "Iteration 51000: loss = 0.001087\n",
            "Iteration 52000: loss = 0.001080\n",
            "Iteration 53000: loss = 0.001072\n",
            "Iteration 54000: loss = 0.001064\n",
            "Iteration 55000: loss = 0.001056\n",
            "Iteration 56000: loss = 0.001048\n",
            "Iteration 57000: loss = 0.001040\n",
            "Iteration 58000: loss = 0.001031\n",
            "Iteration 59000: loss = 0.001022\n",
            "Iteration 60000: loss = 0.001013\n",
            "Iteration 61000: loss = 0.001004\n",
            "Iteration 62000: loss = 0.000995\n",
            "Iteration 63000: loss = 0.000985\n",
            "Iteration 64000: loss = 0.000975\n",
            "Iteration 65000: loss = 0.000965\n",
            "Iteration 66000: loss = 0.000955\n",
            "Iteration 67000: loss = 0.000945\n",
            "Iteration 68000: loss = 0.000934\n",
            "Iteration 69000: loss = 0.000923\n",
            "Iteration 70000: loss = 0.000912\n",
            "Iteration 71000: loss = 0.000900\n",
            "Iteration 72000: loss = 0.000889\n",
            "Iteration 73000: loss = 0.000877\n",
            "Iteration 74000: loss = 0.000865\n",
            "Iteration 75000: loss = 0.000853\n",
            "Iteration 76000: loss = 0.000840\n",
            "Iteration 77000: loss = 0.000828\n",
            "Iteration 78000: loss = 0.000815\n",
            "Iteration 79000: loss = 0.000802\n",
            "Iteration 80000: loss = 0.000789\n",
            "Iteration 81000: loss = 0.000776\n",
            "Iteration 82000: loss = 0.000762\n",
            "Iteration 83000: loss = 0.000749\n",
            "Iteration 84000: loss = 0.000736\n",
            "Iteration 85000: loss = 0.000722\n",
            "Iteration 86000: loss = 0.000708\n",
            "Iteration 87000: loss = 0.000695\n",
            "Iteration 88000: loss = 0.000681\n",
            "Iteration 89000: loss = 0.000668\n",
            "Iteration 90000: loss = 0.000654\n",
            "Iteration 91000: loss = 0.000641\n",
            "Iteration 92000: loss = 0.000627\n",
            "Iteration 93000: loss = 0.000614\n",
            "Iteration 94000: loss = 0.000601\n",
            "Iteration 95000: loss = 0.000587\n",
            "Iteration 96000: loss = 0.000574\n",
            "Iteration 97000: loss = 0.000562\n",
            "Iteration 98000: loss = 0.000549\n",
            "Iteration 99000: loss = 0.000536\n",
            "Iteration 100000: loss = 0.000524\n",
            "Iteration 101000: loss = 0.000512\n",
            "Iteration 102000: loss = 0.000500\n",
            "Iteration 103000: loss = 0.000488\n",
            "Iteration 104000: loss = 0.000476\n",
            "Iteration 105000: loss = 0.000465\n",
            "Iteration 106000: loss = 0.000454\n",
            "Iteration 107000: loss = 0.000443\n",
            "Iteration 108000: loss = 0.000432\n",
            "Iteration 109000: loss = 0.000421\n",
            "Iteration 110000: loss = 0.000411\n",
            "Iteration 111000: loss = 0.000401\n",
            "Iteration 112000: loss = 0.000391\n",
            "Iteration 113000: loss = 0.000382\n",
            "Iteration 114000: loss = 0.000372\n",
            "Iteration 115000: loss = 0.000363\n",
            "Iteration 116000: loss = 0.000354\n",
            "Iteration 117000: loss = 0.000345\n",
            "Iteration 118000: loss = 0.000337\n",
            "Iteration 119000: loss = 0.000329\n",
            "Iteration 120000: loss = 0.000320\n",
            "Iteration 121000: loss = 0.000312\n",
            "Iteration 122000: loss = 0.000305\n",
            "Iteration 123000: loss = 0.000297\n",
            "Iteration 124000: loss = 0.000290\n",
            "Iteration 125000: loss = 0.000283\n",
            "Iteration 126000: loss = 0.000276\n",
            "Iteration 127000: loss = 0.000269\n",
            "Iteration 128000: loss = 0.000263\n",
            "Iteration 129000: loss = 0.000256\n",
            "Iteration 130000: loss = 0.000250\n",
            "Iteration 131000: loss = 0.000244\n",
            "Iteration 132000: loss = 0.000238\n",
            "Iteration 133000: loss = 0.000232\n",
            "Iteration 134000: loss = 0.000227\n",
            "Iteration 135000: loss = 0.000221\n",
            "Iteration 136000: loss = 0.000216\n",
            "Iteration 137000: loss = 0.000211\n",
            "Iteration 138000: loss = 0.000206\n",
            "Iteration 139000: loss = 0.000201\n",
            "Iteration 140000: loss = 0.000196\n",
            "Iteration 141000: loss = 0.000192\n",
            "Iteration 142000: loss = 0.000187\n",
            "Iteration 143000: loss = 0.000183\n",
            "Iteration 144000: loss = 0.000178\n",
            "Iteration 145000: loss = 0.000174\n",
            "Iteration 146000: loss = 0.000170\n",
            "Iteration 147000: loss = 0.000166\n",
            "Iteration 148000: loss = 0.000163\n",
            "Iteration 149000: loss = 0.000159\n",
            "Iteration 150000: loss = 0.000155\n",
            "Iteration 151000: loss = 0.000152\n",
            "Iteration 152000: loss = 0.000148\n",
            "Iteration 153000: loss = 0.000145\n",
            "Iteration 154000: loss = 0.000142\n",
            "Iteration 155000: loss = 0.000138\n",
            "Iteration 156000: loss = 0.000135\n",
            "Iteration 157000: loss = 0.000132\n",
            "Iteration 158000: loss = 0.000129\n",
            "Iteration 159000: loss = 0.000126\n",
            "Iteration 160000: loss = 0.000124\n",
            "Iteration 161000: loss = 0.000121\n",
            "Iteration 162000: loss = 0.000118\n",
            "Iteration 163000: loss = 0.000116\n",
            "Iteration 164000: loss = 0.000113\n",
            "Iteration 165000: loss = 0.000111\n",
            "Iteration 166000: loss = 0.000108\n",
            "Iteration 167000: loss = 0.000106\n",
            "Iteration 168000: loss = 0.000103\n",
            "Iteration 169000: loss = 0.000101\n",
            "Iteration 170000: loss = 0.000099\n",
            "Iteration 171000: loss = 0.000097\n",
            "Iteration 172000: loss = 0.000095\n",
            "Iteration 173000: loss = 0.000093\n",
            "Iteration 174000: loss = 0.000091\n",
            "Iteration 175000: loss = 0.000089\n",
            "Iteration 176000: loss = 0.000087\n",
            "Iteration 177000: loss = 0.000085\n",
            "Iteration 178000: loss = 0.000083\n",
            "Iteration 179000: loss = 0.000082\n",
            "Iteration 180000: loss = 0.000080\n",
            "Iteration 181000: loss = 0.000078\n",
            "Iteration 182000: loss = 0.000077\n",
            "Iteration 183000: loss = 0.000075\n",
            "Iteration 184000: loss = 0.000073\n",
            "Iteration 185000: loss = 0.000072\n",
            "Iteration 186000: loss = 0.000070\n",
            "Iteration 187000: loss = 0.000069\n",
            "Iteration 188000: loss = 0.000068\n",
            "Iteration 189000: loss = 0.000066\n",
            "Iteration 190000: loss = 0.000065\n",
            "Iteration 191000: loss = 0.000064\n",
            "Iteration 192000: loss = 0.000062\n",
            "Iteration 193000: loss = 0.000061\n",
            "Iteration 194000: loss = 0.000060\n",
            "Iteration 195000: loss = 0.000059\n",
            "Iteration 196000: loss = 0.000057\n",
            "Iteration 197000: loss = 0.000056\n",
            "Iteration 198000: loss = 0.000055\n",
            "Iteration 199000: loss = 0.000054\n",
            "Iteration 200000: loss = 0.000053\n",
            "Iteration 201000: loss = 0.000052\n",
            "Iteration 202000: loss = 0.000051\n",
            "Iteration 203000: loss = 0.000050\n",
            "Iteration 204000: loss = 0.000049\n",
            "Iteration 205000: loss = 0.000048\n",
            "Iteration 206000: loss = 0.000047\n",
            "Iteration 207000: loss = 0.000046\n",
            "Iteration 208000: loss = 0.000045\n",
            "Iteration 209000: loss = 0.000044\n",
            "Iteration 210000: loss = 0.000043\n",
            "Iteration 211000: loss = 0.000042\n",
            "Iteration 212000: loss = 0.000042\n",
            "Iteration 213000: loss = 0.000041\n",
            "Iteration 214000: loss = 0.000040\n",
            "Iteration 215000: loss = 0.000039\n",
            "Iteration 216000: loss = 0.000038\n",
            "Iteration 217000: loss = 0.000038\n",
            "Iteration 218000: loss = 0.000037\n",
            "Iteration 219000: loss = 0.000036\n",
            "Iteration 220000: loss = 0.000036\n",
            "Iteration 221000: loss = 0.000035\n",
            "Iteration 222000: loss = 0.000034\n",
            "Iteration 223000: loss = 0.000034\n",
            "Iteration 224000: loss = 0.000033\n",
            "Iteration 225000: loss = 0.000032\n",
            "Iteration 226000: loss = 0.000032\n",
            "Iteration 227000: loss = 0.000031\n",
            "Iteration 228000: loss = 0.000030\n",
            "Iteration 229000: loss = 0.000030\n",
            "Iteration 230000: loss = 0.000029\n",
            "Iteration 231000: loss = 0.000029\n",
            "Iteration 232000: loss = 0.000028\n",
            "Iteration 233000: loss = 0.000028\n",
            "Iteration 234000: loss = 0.000027\n",
            "Iteration 235000: loss = 0.000027\n",
            "Iteration 236000: loss = 0.000026\n",
            "Iteration 237000: loss = 0.000026\n",
            "Iteration 238000: loss = 0.000025\n",
            "Iteration 239000: loss = 0.000025\n",
            "Iteration 240000: loss = 0.000024\n",
            "Iteration 241000: loss = 0.000024\n",
            "Iteration 242000: loss = 0.000023\n",
            "Iteration 243000: loss = 0.000023\n",
            "Iteration 244000: loss = 0.000022\n",
            "Iteration 245000: loss = 0.000022\n",
            "Iteration 246000: loss = 0.000022\n",
            "Iteration 247000: loss = 0.000021\n",
            "Iteration 248000: loss = 0.000021\n",
            "Iteration 249000: loss = 0.000020\n",
            "Iteration 250000: loss = 0.000020\n",
            "Iteration 251000: loss = 0.000020\n",
            "Iteration 252000: loss = 0.000019\n",
            "Iteration 253000: loss = 0.000019\n",
            "Iteration 254000: loss = 0.000019\n",
            "Iteration 255000: loss = 0.000018\n",
            "Iteration 256000: loss = 0.000018\n",
            "Iteration 257000: loss = 0.000018\n",
            "Iteration 258000: loss = 0.000017\n",
            "Iteration 259000: loss = 0.000017\n",
            "Iteration 260000: loss = 0.000017\n",
            "Iteration 261000: loss = 0.000016\n",
            "Iteration 262000: loss = 0.000016\n",
            "Iteration 263000: loss = 0.000016\n",
            "Iteration 264000: loss = 0.000015\n",
            "Iteration 265000: loss = 0.000015\n",
            "Iteration 266000: loss = 0.000015\n",
            "Iteration 267000: loss = 0.000015\n",
            "Iteration 268000: loss = 0.000014\n",
            "Iteration 269000: loss = 0.000014\n",
            "Iteration 270000: loss = 0.000014\n",
            "Iteration 271000: loss = 0.000014\n",
            "Iteration 272000: loss = 0.000013\n",
            "Iteration 273000: loss = 0.000013\n",
            "Iteration 274000: loss = 0.000013\n",
            "Iteration 275000: loss = 0.000013\n",
            "Iteration 276000: loss = 0.000012\n",
            "Iteration 277000: loss = 0.000012\n",
            "Iteration 278000: loss = 0.000012\n",
            "Iteration 279000: loss = 0.000012\n",
            "Iteration 280000: loss = 0.000012\n",
            "Iteration 281000: loss = 0.000011\n",
            "Iteration 282000: loss = 0.000011\n",
            "Iteration 283000: loss = 0.000011\n",
            "Iteration 284000: loss = 0.000011\n",
            "Iteration 285000: loss = 0.000011\n",
            "Iteration 286000: loss = 0.000010\n",
            "Iteration 287000: loss = 0.000010\n",
            "Iteration 288000: loss = 0.000010\n",
            "Operation terminated successfully.\n",
            "         Iterations: 288142\n",
            "         Final loss function: 0.000010\n"
          ]
        }
      ],
      "source": [
        "NN = Neural_Network()\n",
        "T = Trainer(NN)\n",
        "T.train(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stVg-EHWokMs"
      },
      "source": [
        "### Predicted Train values vs Actual Train values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcD-NUn2okMt",
        "outputId": "746095e5-1398-47f5-bc42-c4d1c7978a9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[0.75000153],\n",
              "        [0.82215244],\n",
              "        [0.92607993]]),\n",
              " array([[0.75],\n",
              "        [0.82],\n",
              "        [0.93]]))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "NN.forward(X), y"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dip",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
