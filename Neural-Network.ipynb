{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCDECMIEokMX"
      },
      "source": [
        "```\n",
        "Input: (Hours Slept, Hours Studied)\n",
        "Output: (Test Score)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zh8uuf6dokMe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import optimize\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o42_ALbJokMh"
      },
      "outputs": [],
      "source": [
        "X = np.array(([3,5], [5,1], [10,2]), dtype=float)\n",
        "y = np.array(([75], [82], [93]), dtype=float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7oQkh8iokMi"
      },
      "source": [
        "### Scaling data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Phc4EtZHokMi"
      },
      "outputs": [],
      "source": [
        "X = X / np.amax(X, axis=0)\n",
        "y = y / 100 # Max test score is 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmB-EwyFokMk"
      },
      "outputs": [],
      "source": [
        "class Neural_Network(object):\n",
        "    def __init__(self, inputLayerSize: int = 2, hiddenLayerSize: int = 3, outputLayerSize: int = 1):\n",
        "        # Define Hyperparameters\n",
        "        self.inputLayerSize = inputLayerSize\n",
        "        self.outputLayerSize = outputLayerSize\n",
        "        self.hiddenLayerSize = hiddenLayerSize\n",
        "\n",
        "        # Weights (parameters)\n",
        "        self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
        "        self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
        "\n",
        "    def forward(self, X):\n",
        "        '''\n",
        "        Propagate inputs though network\n",
        "        Returns a new array with the same shape as target array y.\n",
        "\n",
        "        z2 = X.W1 where X is input matrix and W1 is weight matrix from input to hidden layer\n",
        "        a2 = f(z2) where f is sigmoid activation function\n",
        "        z3 = a2.W2 where a2 is hidden layer output matrix and W2 is weight matrix from hidden to output layer\n",
        "        yHat = f(z3) where f is sigmoid activation function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray\n",
        "            Input data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        out : ndarray\n",
        "            Predicted data.\n",
        "        '''\n",
        "        self.z2 = np.dot(X, self.W1)        # dot product of X (input) and first set of 2x3 weights\n",
        "        self.a2 = self.sigmoid(self.z2)     # apply sigmoid activation function to z2\n",
        "        self.z3 = np.dot(self.a2, self.W2)  # dot product of hidden layer (a2) and second set of 3x1 weights\n",
        "        yHat = self.sigmoid(self.z3)        # apply sigmoid activation function to z3\n",
        "        return yHat\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        '''\n",
        "        Apply sigmoid activation function to scalar, vector, or matrix\n",
        "        '''\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def sigmoidPrime(self, z):\n",
        "        '''\n",
        "        Derivative of sigmoid function\n",
        "        Returns the derivative of the sigmoid function evaluated at z\n",
        "        '''\n",
        "        return np.exp(-z)/((1 + np.exp(-z))**2)\n",
        "\n",
        "    def costFunction(self, X, y):\n",
        "        \"\"\"\n",
        "        Computes cost for given X,y using weights already stored in class.\n",
        "        Returns a new array with the same shape as y.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray\n",
        "            Input data.\n",
        "        y : ndarray\n",
        "            Target data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        out : ndarray\n",
        "            Cost of the predicted data with respect to true target data.\n",
        "\n",
        "        \"\"\"\n",
        "        self.yHat = self.forward(X)\n",
        "        J = 0.5* sum((y-self.yHat)**2)\n",
        "        return J\n",
        "\n",
        "    def costFunctionPrime(self, X, y):\n",
        "        \"\"\"\n",
        "        Computes derivative with respect to W1 and W2 for a given X and y\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray\n",
        "            Input data.\n",
        "        y : ndarray\n",
        "            Target data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        out : Tuple[ndarray, ndarray]\n",
        "            Derivative of cost function with respect to W1 and W2 respectively.\n",
        "        \"\"\"\n",
        "        self.yHat = self.forward(X)\n",
        "\n",
        "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
        "        dJdW2 = np.dot(self.a2.T, delta3)\n",
        "\n",
        "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
        "        dJdW1 = np.dot(X.T, delta2)\n",
        "\n",
        "        return dJdW1, dJdW2\n",
        "\n",
        "    #Helper Functions for interacting with other classes:\n",
        "    def getParams(self):\n",
        "        #Get W1 and W2 unrolled into vector:\n",
        "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
        "        return params\n",
        "\n",
        "    def setParams(self, params):\n",
        "        #Set W1 and W2 using single paramater vector.\n",
        "        W1_start = 0\n",
        "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
        "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
        "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
        "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
        "\n",
        "    def computeGradients(self, X, y):\n",
        "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
        "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZEcFozxokMo"
      },
      "outputs": [],
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, N):\n",
        "        # Make Local reference to network:\n",
        "        self.N = N\n",
        "\n",
        "    def train(self, X, y):\n",
        "        #Make an internal variable for the callback function:\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        #Make empty list to store costs:\n",
        "        self.J = []\n",
        "\n",
        "        loss = self.N.costFunction(X, y)\n",
        "        k = 0\n",
        "        maxiter =  300000\n",
        "        lr = 0.06\n",
        "\n",
        "        while (loss > 1e-5) and (k < maxiter):\n",
        "            grad = self.N.computeGradients(X, y)\n",
        "            nW1s = self.N.inputLayerSize * self.N.hiddenLayerSize\n",
        "            dJW1 = grad[:nW1s].reshape(self.N.inputLayerSize, self.N.hiddenLayerSize)\n",
        "            dJW2 = grad[nW1s:].reshape(self.N.hiddenLayerSize, self.N.outputLayerSize)\n",
        "            self.N.W1 -= lr * dJW1\n",
        "            self.N.W2 -= lr * dJW2\n",
        "            loss = self.N.costFunction(X, y)\n",
        "            self.J.append(loss)\n",
        "            k += 1\n",
        "            if k % 1000 == 0:\n",
        "                print(\"Iteration %d: loss = %f\" % (k, loss[0]))\n",
        "\n",
        "        print(\"Operation terminated successfully.\")\n",
        "        print(\"         Iterations: %d\" % k)\n",
        "        print(\"         Final loss function: %f\" % loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLGiMHFFokMq",
        "outputId": "e0bd2411-31d2-4932-a543-66490e330239",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1000: loss = 0.003585\n",
            "Iteration 2000: loss = 0.002906\n",
            "Iteration 3000: loss = 0.002498\n",
            "Iteration 4000: loss = 0.002209\n",
            "Iteration 5000: loss = 0.001999\n",
            "Iteration 6000: loss = 0.001840\n",
            "Iteration 7000: loss = 0.001715\n",
            "Iteration 8000: loss = 0.001612\n",
            "Iteration 9000: loss = 0.001526\n",
            "Iteration 10000: loss = 0.001450\n",
            "Iteration 11000: loss = 0.001383\n",
            "Iteration 12000: loss = 0.001322\n",
            "Iteration 13000: loss = 0.001266\n",
            "Iteration 14000: loss = 0.001215\n",
            "Iteration 15000: loss = 0.001167\n",
            "Iteration 16000: loss = 0.001122\n",
            "Iteration 17000: loss = 0.001080\n",
            "Iteration 18000: loss = 0.001041\n",
            "Iteration 19000: loss = 0.001003\n",
            "Iteration 20000: loss = 0.000968\n",
            "Iteration 21000: loss = 0.000935\n",
            "Iteration 22000: loss = 0.000903\n",
            "Iteration 23000: loss = 0.000873\n",
            "Iteration 24000: loss = 0.000845\n",
            "Iteration 25000: loss = 0.000817\n",
            "Iteration 26000: loss = 0.000791\n",
            "Iteration 27000: loss = 0.000767\n",
            "Iteration 28000: loss = 0.000743\n",
            "Iteration 29000: loss = 0.000720\n",
            "Iteration 30000: loss = 0.000699\n",
            "Iteration 31000: loss = 0.000678\n",
            "Iteration 32000: loss = 0.000658\n",
            "Iteration 33000: loss = 0.000639\n",
            "Iteration 34000: loss = 0.000621\n",
            "Iteration 35000: loss = 0.000603\n",
            "Iteration 36000: loss = 0.000586\n",
            "Iteration 37000: loss = 0.000570\n",
            "Iteration 38000: loss = 0.000554\n",
            "Iteration 39000: loss = 0.000539\n",
            "Iteration 40000: loss = 0.000524\n",
            "Iteration 41000: loss = 0.000510\n",
            "Iteration 42000: loss = 0.000496\n",
            "Iteration 43000: loss = 0.000483\n",
            "Iteration 44000: loss = 0.000470\n",
            "Iteration 45000: loss = 0.000458\n",
            "Iteration 46000: loss = 0.000446\n",
            "Iteration 47000: loss = 0.000435\n",
            "Iteration 48000: loss = 0.000424\n",
            "Iteration 49000: loss = 0.000413\n",
            "Iteration 50000: loss = 0.000402\n",
            "Iteration 51000: loss = 0.000392\n",
            "Iteration 52000: loss = 0.000382\n",
            "Iteration 53000: loss = 0.000373\n",
            "Iteration 54000: loss = 0.000364\n",
            "Iteration 55000: loss = 0.000355\n",
            "Iteration 56000: loss = 0.000346\n",
            "Iteration 57000: loss = 0.000338\n",
            "Iteration 58000: loss = 0.000329\n",
            "Iteration 59000: loss = 0.000321\n",
            "Iteration 60000: loss = 0.000314\n",
            "Iteration 61000: loss = 0.000306\n",
            "Iteration 62000: loss = 0.000299\n",
            "Iteration 63000: loss = 0.000292\n",
            "Iteration 64000: loss = 0.000285\n",
            "Iteration 65000: loss = 0.000278\n",
            "Iteration 66000: loss = 0.000272\n",
            "Iteration 67000: loss = 0.000266\n",
            "Iteration 68000: loss = 0.000259\n",
            "Iteration 69000: loss = 0.000254\n",
            "Iteration 70000: loss = 0.000248\n",
            "Iteration 71000: loss = 0.000242\n",
            "Iteration 72000: loss = 0.000237\n",
            "Iteration 73000: loss = 0.000231\n",
            "Iteration 74000: loss = 0.000226\n",
            "Iteration 75000: loss = 0.000221\n",
            "Iteration 76000: loss = 0.000216\n",
            "Iteration 77000: loss = 0.000211\n",
            "Iteration 78000: loss = 0.000206\n",
            "Iteration 79000: loss = 0.000202\n",
            "Iteration 80000: loss = 0.000197\n",
            "Iteration 81000: loss = 0.000193\n",
            "Iteration 82000: loss = 0.000189\n",
            "Iteration 83000: loss = 0.000185\n",
            "Iteration 84000: loss = 0.000181\n",
            "Iteration 85000: loss = 0.000177\n",
            "Iteration 86000: loss = 0.000173\n",
            "Iteration 87000: loss = 0.000169\n",
            "Iteration 88000: loss = 0.000165\n",
            "Iteration 89000: loss = 0.000162\n",
            "Iteration 90000: loss = 0.000158\n",
            "Iteration 91000: loss = 0.000155\n",
            "Iteration 92000: loss = 0.000152\n",
            "Iteration 93000: loss = 0.000149\n",
            "Iteration 94000: loss = 0.000145\n",
            "Iteration 95000: loss = 0.000142\n",
            "Iteration 96000: loss = 0.000139\n",
            "Iteration 97000: loss = 0.000136\n",
            "Iteration 98000: loss = 0.000134\n",
            "Iteration 99000: loss = 0.000131\n",
            "Iteration 100000: loss = 0.000128\n",
            "Iteration 101000: loss = 0.000125\n",
            "Iteration 102000: loss = 0.000123\n",
            "Iteration 103000: loss = 0.000120\n",
            "Iteration 104000: loss = 0.000118\n",
            "Iteration 105000: loss = 0.000115\n",
            "Iteration 106000: loss = 0.000113\n",
            "Iteration 107000: loss = 0.000111\n",
            "Iteration 108000: loss = 0.000108\n",
            "Iteration 109000: loss = 0.000106\n",
            "Iteration 110000: loss = 0.000104\n",
            "Iteration 111000: loss = 0.000102\n",
            "Iteration 112000: loss = 0.000100\n",
            "Iteration 113000: loss = 0.000098\n",
            "Iteration 114000: loss = 0.000096\n",
            "Iteration 115000: loss = 0.000094\n",
            "Iteration 116000: loss = 0.000092\n",
            "Iteration 117000: loss = 0.000090\n",
            "Iteration 118000: loss = 0.000088\n",
            "Iteration 119000: loss = 0.000087\n",
            "Iteration 120000: loss = 0.000085\n",
            "Iteration 121000: loss = 0.000083\n",
            "Iteration 122000: loss = 0.000082\n",
            "Iteration 123000: loss = 0.000080\n",
            "Iteration 124000: loss = 0.000078\n",
            "Iteration 125000: loss = 0.000077\n",
            "Iteration 126000: loss = 0.000075\n",
            "Iteration 127000: loss = 0.000074\n",
            "Iteration 128000: loss = 0.000072\n",
            "Iteration 129000: loss = 0.000071\n",
            "Iteration 130000: loss = 0.000070\n",
            "Iteration 131000: loss = 0.000068\n",
            "Iteration 132000: loss = 0.000067\n",
            "Iteration 133000: loss = 0.000066\n",
            "Iteration 134000: loss = 0.000064\n",
            "Iteration 135000: loss = 0.000063\n",
            "Iteration 136000: loss = 0.000062\n",
            "Iteration 137000: loss = 0.000061\n",
            "Iteration 138000: loss = 0.000059\n",
            "Iteration 139000: loss = 0.000058\n",
            "Iteration 140000: loss = 0.000057\n",
            "Iteration 141000: loss = 0.000056\n",
            "Iteration 142000: loss = 0.000055\n",
            "Iteration 143000: loss = 0.000054\n",
            "Iteration 144000: loss = 0.000053\n",
            "Iteration 145000: loss = 0.000052\n",
            "Iteration 146000: loss = 0.000051\n",
            "Iteration 147000: loss = 0.000050\n",
            "Iteration 148000: loss = 0.000049\n",
            "Iteration 149000: loss = 0.000048\n",
            "Iteration 150000: loss = 0.000047\n",
            "Iteration 151000: loss = 0.000046\n",
            "Iteration 152000: loss = 0.000045\n",
            "Iteration 153000: loss = 0.000045\n",
            "Iteration 154000: loss = 0.000044\n",
            "Iteration 155000: loss = 0.000043\n",
            "Iteration 156000: loss = 0.000042\n",
            "Iteration 157000: loss = 0.000041\n",
            "Iteration 158000: loss = 0.000041\n",
            "Iteration 159000: loss = 0.000040\n",
            "Iteration 160000: loss = 0.000039\n",
            "Iteration 161000: loss = 0.000038\n",
            "Iteration 162000: loss = 0.000038\n",
            "Iteration 163000: loss = 0.000037\n",
            "Iteration 164000: loss = 0.000036\n",
            "Iteration 165000: loss = 0.000036\n",
            "Iteration 166000: loss = 0.000035\n",
            "Iteration 167000: loss = 0.000034\n",
            "Iteration 168000: loss = 0.000034\n",
            "Iteration 169000: loss = 0.000033\n",
            "Iteration 170000: loss = 0.000032\n",
            "Iteration 171000: loss = 0.000032\n",
            "Iteration 172000: loss = 0.000031\n",
            "Iteration 173000: loss = 0.000031\n",
            "Iteration 174000: loss = 0.000030\n",
            "Iteration 175000: loss = 0.000030\n",
            "Iteration 176000: loss = 0.000029\n",
            "Iteration 177000: loss = 0.000028\n",
            "Iteration 178000: loss = 0.000028\n",
            "Iteration 179000: loss = 0.000027\n",
            "Iteration 180000: loss = 0.000027\n",
            "Iteration 181000: loss = 0.000026\n",
            "Iteration 182000: loss = 0.000026\n",
            "Iteration 183000: loss = 0.000025\n",
            "Iteration 184000: loss = 0.000025\n",
            "Iteration 185000: loss = 0.000025\n",
            "Iteration 186000: loss = 0.000024\n",
            "Iteration 187000: loss = 0.000024\n",
            "Iteration 188000: loss = 0.000023\n",
            "Iteration 189000: loss = 0.000023\n",
            "Iteration 190000: loss = 0.000022\n",
            "Iteration 191000: loss = 0.000022\n",
            "Iteration 192000: loss = 0.000022\n",
            "Iteration 193000: loss = 0.000021\n",
            "Iteration 194000: loss = 0.000021\n",
            "Iteration 195000: loss = 0.000020\n",
            "Iteration 196000: loss = 0.000020\n",
            "Iteration 197000: loss = 0.000020\n",
            "Iteration 198000: loss = 0.000019\n",
            "Iteration 199000: loss = 0.000019\n",
            "Iteration 200000: loss = 0.000019\n",
            "Iteration 201000: loss = 0.000018\n",
            "Iteration 202000: loss = 0.000018\n",
            "Iteration 203000: loss = 0.000018\n",
            "Iteration 204000: loss = 0.000017\n",
            "Iteration 205000: loss = 0.000017\n",
            "Iteration 206000: loss = 0.000017\n",
            "Iteration 207000: loss = 0.000017\n",
            "Iteration 208000: loss = 0.000016\n",
            "Iteration 209000: loss = 0.000016\n",
            "Iteration 210000: loss = 0.000016\n",
            "Iteration 211000: loss = 0.000015\n",
            "Iteration 212000: loss = 0.000015\n",
            "Iteration 213000: loss = 0.000015\n",
            "Iteration 214000: loss = 0.000015\n",
            "Iteration 215000: loss = 0.000014\n",
            "Iteration 216000: loss = 0.000014\n",
            "Iteration 217000: loss = 0.000014\n",
            "Iteration 218000: loss = 0.000014\n",
            "Iteration 219000: loss = 0.000013\n",
            "Iteration 220000: loss = 0.000013\n",
            "Iteration 221000: loss = 0.000013\n",
            "Iteration 222000: loss = 0.000013\n",
            "Iteration 223000: loss = 0.000012\n",
            "Iteration 224000: loss = 0.000012\n",
            "Iteration 225000: loss = 0.000012\n",
            "Iteration 226000: loss = 0.000012\n",
            "Iteration 227000: loss = 0.000012\n",
            "Iteration 228000: loss = 0.000011\n",
            "Iteration 229000: loss = 0.000011\n",
            "Iteration 230000: loss = 0.000011\n",
            "Iteration 231000: loss = 0.000011\n",
            "Iteration 232000: loss = 0.000011\n",
            "Iteration 233000: loss = 0.000010\n",
            "Iteration 234000: loss = 0.000010\n",
            "Iteration 235000: loss = 0.000010\n",
            "Operation terminated successfully.\n",
            "         Iterations: 235344\n",
            "         Final loss function: 0.000010\n"
          ]
        }
      ],
      "source": [
        "NN = Neural_Network()\n",
        "T = Trainer(NN)\n",
        "T.train(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stVg-EHWokMs"
      },
      "source": [
        "### Predicted Train values vs Actual Train values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcD-NUn2okMt",
        "outputId": "746095e5-1398-47f5-bc42-c4d1c7978a9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.7500808 ],\n",
              "        [0.82209844],\n",
              "        [0.92605158]]),\n",
              " array([[0.75],\n",
              "        [0.82],\n",
              "        [0.93]]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "NN.forward(X), y"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dip",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}