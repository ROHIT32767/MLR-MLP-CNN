{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAAD0CAYAAACvgrpiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAjkklEQVR4nO3de5TN9f7H8fceYZhhJEquQ6IUKfOjaOVSlMKhjMvpgi7KidBquiOUKJe0IqeUdMhQjhydE6UmziG6os4sUsS4VC7RuCUz398frd7n8x178509+/L97Hk+1mqt1/e299vUJz6+7+/nG3AcxxEAAAAAACyVFO8CAAAAAAAoCSa2AAAAAACrMbEFAAAAAFiNiS0AAAAAwGpMbAEAAAAAVmNiCwAAAACwGhNbAAAAAIDVmNgCAAAAAKzGxBYAAAAAYDUmtgAAAAAAqzGxDdNrr70mgUBAPvvss3iXElUvvviiZGZmSt26dSUQCEj//v3jXRIQUaVhLOfl5cno0aOlZcuWcuaZZ0q1atWkXbt2snz58niXBkREaRjHR48elTvuuEMuvvhiSUtLk9TUVLnkkktk6tSp8ttvv8W7PCAiSsNYLuo///mPBAIBCQQCsnfv3niXY7Uz4l0A/G3ChAmSn58vLVu2lN27d8e7HABhWLx4sUyYMEG6d+8u/fr1kxMnTsjrr78uHTt2lFdffVUGDBgQ7xIBnMbRo0flv//9r1x//fWSnp4uSUlJsnr1ahk+fLisXbtW3njjjXiXCKCYCgsLZciQIZKSkiKHDx+OdznWY2KLU1qxYoXerU1NTY13OQDC0L59e9m+fbtUq1ZN991zzz3SvHlzGTlyJBNbwAJVq1aVNWvWuPbdc889kpaWJi+88IJMnjxZatSoEafqAITjpZdekry8PLnzzjtl6tSp8S7HerQiR1D//v0lNTVVtm/fLl26dJHU1FSpVauWTJs2TUREvvrqK+nQoYOkpKRIvXr1Tvrb1f3798sDDzwgTZs2ldTUVKlcubJ07txZ1q9ff9J3bdu2Tbp16yYpKSly9tlny/Dhw2XZsmUSCATko48+cp27du1aue666yQtLU0qVqwobdu2lVWrVnn6NdWrV08CgUB4PxDAUok2li+66CLXpFZEpHz58nL99dfLjh07JD8/v5g/IcD/Em0ch5Keni4iIgcOHAj7MwA/S9SxvH//fnn88cdlzJgxUqVKlWL/XHAyJrYRVlBQIJ07d5Y6derIM888I+np6TJ48GB57bXX5LrrrpOMjAyZMGGCVKpUSW677TbZunWrXrtlyxZ5++23pUuXLjJ58mTJysqSr776Stq2bSu7du3S8w4fPiwdOnSQ5cuXy3333SePPfaYrF69Wh566KGT6vnwww/lqquukl9++UVGjRol48aNkwMHDkiHDh3kk08+icnPBLBRaRjLP/zwg1SsWFEqVqwY1vWA3yXiOD5+/Ljs3btX8vLyZNGiRTJx4kSpV6+eNGzYsOQ/MMCnEnEsjxgxQmrUqCF33313yX9A+J2DsMyaNcsREefTTz/Vff369XNExBk3bpzu+/nnn50KFSo4gUDAyc7O1v0bN250RMQZNWqU7jt27JhTUFDg+p6tW7c65cuXd8aMGaP7Jk2a5IiI8/bbb+u+o0ePOhdccIEjIk5OTo7jOI5TWFjonH/++c61117rFBYW6rlHjhxx6tev73Ts2LFYv+aUlBSnX79+xboG8LvSOJYdx3E2b97sJCcnO7feemuxrwX8pjSN43nz5jkiov9kZGQ4GzZs8HQt4HelZSyvX7/eKVOmjLNs2TLHcRxn1KhRjog4e/bsOe21CI07tlFw5513aq5SpYo0btxYUlJSpFevXrq/cePGUqVKFdmyZYvuK1++vCQl/f6vpKCgQPbt2yepqanSuHFj+eKLL/S8pUuXSq1ataRbt266Lzk5We666y5XHevWrZPNmzfLn//8Z9m3b5/s3btX9u7dK4cPH5arr75aVq5cKYWFhRH/9QOJIlHH8pEjRyQzM1MqVKgg48eP9/4DASyUaOO4ffv28v7778ubb74p99xzj5QtW5ZFZ1AqJNJYvu+++6Rz587SqVOn8H4YCIrFoyIsOTlZqlev7tqXlpYmtWvXPulZ1bS0NPn55591u7CwUKZOnSrTp0+XrVu3SkFBgR4766yzNG/btk3OO++8kz6vaBvS5s2bRUSkX79+Ies9ePCgnHnmmR5/dUDpkahjuaCgQPr06SO5ubny7rvvSs2aNU97DWCrRBzH55xzjpxzzjkiItKzZ08ZN26cdOzYUTZv3sziUUhYiTSW58+fL6tXr5avv/465PUIDxPbCCtTpkyx9juOo3ncuHEyYsQIuf3222Xs2LFStWpVSUpKkmHDhoV1Z/WPa5599llp3rx50HNY6RgILlHH8l133SXvvPOOzJ07Vzp06FDsWgCbJOo4NvXs2VMee+wxWbx4Mc/qIWEl0ljOysqSzMxMKVeunHz//fci8r/F3/Ly8uT48eP8pXOYmNj6yFtvvSXt27eXV155xbX/wIEDrhVN69WrJ7m5ueI4jutvlb799lvXdeedd56IiFSuXFmuueaaKFYOwOTXsZyVlSWzZs2S5557Tvr27Rv25wClgV/HcVFHjx4Vkd/vEAE4md/Gcl5enrzxxhtB3z192WWXySWXXCLr1q0r9ueCVZF9pUyZMq6/YRIRefPNN2Xnzp2ufddee63s3LlT/vGPf+i+Y8eOycsvv+w6r0WLFnLeeefJxIkT5dChQyd93549eyJYPYA/+HEsP/vsszJx4kR59NFHZejQocX55QClkt/G8d69e0+qR0Rk5syZIiKSkZFx6l8QUEr5bSwvWrTopH969+4tIiKvv/66TJkypVi/PvwPd2x9pEuXLjJmzBgZMGCAtG7dWr766iuZO3euNGjQwHXe3XffLS+88IL07dtXhg4dKueee67MnTtXkpOTRUT0b5mSkpJk5syZ0rlzZ7noootkwIABUqtWLdm5c6fk5ORI5cqVZcmSJaesacmSJfqer99++002bNggTz75pIiIdOvWTZo1axbpHwNgPb+N5UWLFsmDDz4o559/vlx44YUyZ84c1/GOHTvqM3sAfue3cTxnzhyZMWOGdO/eXRo0aCD5+fmybNkyef/996Vr1648WgCE4Lex3L1795P2/XGHtnPnzie9dx7eMbH1kUcffVQOHz4sb7zxhsyfP18uu+wy+ec//ykPP/yw67zU1FT58MMPZciQITJ16lRJTU2V2267TVq3bi033XSTDkARkXbt2snHH38sY8eOlRdeeEEOHTokNWrUkFatWnl6FmfhwoUye/Zs3f7yyy/lyy+/FBGR2rVrM7EFgvDbWP7jL6c2b94st95660nHc3JymNgCRfhtHF955ZWyevVqmTdvnvz4449yxhlnSOPGjWXy5MkyZMiQqPwMgETgt7GM6Ak4wfpaYKXnnntOhg8fLjt27JBatWrFuxwAYWIsA/ZjHAOJgbFsDya2ljp69KhUqFBBt48dOyaXXnqpFBQUyDfffBPHygAUB2MZsB/jGEgMjGW70YpsqRtvvFHq1q0rzZs3l4MHD8qcOXNk48aNMnfu3HiXBqAYGMuA/RjHQGJgLNuNia2lrr32Wpk5c6bMnTtXCgoKpEmTJpKdna2rqgGwA2MZsB/jGEgMjGW70YoMAAAAALAa77EFAAAAAFiNiS0AAAAAwGpMbAEAAAAAVvO8eFQgEIhmHUDC8Ptj64xlwBs/j2XGMeCNn8exCGMZ8MrLWOaOLQAAAADAakxsAQAAAABWY2ILAAAAALAaE1sAAAAAgNWY2AIAAAAArMbEFgAAAABgNSa2AAAAAACrMbEFAAAAAFiNiS0AAAAAwGpMbAEAAAAAVmNiCwAAAACwGhNbAAAAAIDVmNgCAAAAAKzGxBYAAAAAYDUmtgAAAAAAqzGxBQAAAABYjYktAAAAAMBqZ8S7AAAAAPhTtWrVXNs5OTmat27dqrlbt24xqwkAguGOLQAAAADAakxsAQAAAABWoxUZAAAAQY0YMcK1ff7552sePXp0rMsBgJC4YwsAAAAAsBoTWwAAAACA1QKO4zieTgwEol0LkBA8Dqm4YSwD3vh5LDOOEU3p6emac3NzXceWL1+u2YaVkP08jkUYy4BXXsYyd2wBAAAAAFZjYgsAAAAAsBoTWwAAAACA1XjdDwAAAFT79u01Jycnu449//zzsS4HADzhji0AAAAAwGpMbAEAAAAAVuN1P0CE8WoBIDH4eSz7dRy3a9cuaG7btm3I80Ix22E/+uijElaG0ylfvrzmdevWaf71119d52VkZGg+ceJE1OsqKT+PYxH/jmXAb3jdDwAAAAAg4TGxBQAAAABYjVZkIMJoewKKp2HDhpo3bdoU9JykJPffw06fPl1zVlaW5iNHjkSsLj+P5XiP41j/bEaPHq35iSeeiOl3lxZ16tTRvH37ds2ffvqp67yWLVvGrKZI8PM4Fon/WEbiyczM1Jydna35wIEDrvM6duyo+Ysvvoh6XSVFKzIAAAAAIOExsQUAAAAAWO2MeBcAACh9evbsqblLly6aCwsLPV1/1llnaS5btmzkCkNIoVqAzRWLzZWMzfPDWRXZNGrUqJCfZX4nwlerVi3NZsvfzp0741EOfKRXr16ahw4d6jrWunVrzeb/v/v27at59erVmnfs2BGNEhGCOZbT0tJcx6666irNNrQie8EdWwAAAACA1ZjYAgAAAACsRisyfK9Hjx6a//73vwc9p3///ppnz54d7ZIAhCElJUWz2X588803F/uzmjRporlixYqaDx48GGZ1KKpou7DZDmxasWJF0P1eVy8O1bIcql256H7zelZMDl+o1Y6ff/75GFcCPxg+fLjmiRMnai76uIi5beZ58+ZpNluZaUWOviFDhsS7hLjhji0AAAAAwGpMbAEAAAAAVqMVGb5Xrlw5zaFWTM3IyNBMKzLgT+YKjOG0H5v++te/at69e3eJPgvBhWo9FnGvhFzS9l/zerPN2OvKyWadZl1mRvgYX4nt8ssv11y3bl3N5sr1SUmh74MFAoGg55n7FyxYoLlNmzaa16xZE0bFCOaWW27RfOmllwY9p6CgwLWdn58f1ZrigTu2AAAAAACrMbEFAAAAAFiNiS0AAAAAwGoBx3EcTycavfJANJnP1IqIfPLJJ5qbNm0a9Jr69etr3r59e3QK88jjkIobxjLixRzLoZ4BCqVr166u7dzcXM3RGvN+HsvRGsfm866nesa2ffv2mmPxLGtOTo5mr8/exrpGGyUnJ2t++OGHNZv/7rt37+66ZvHixVGvK5L8PI5F4vN7svlcbXZ2tuY6depoNtc0MZ+dLbrWSahjofbv2rVLc+/evV2fxTO34TOfnw3133xeXp5r2/yzsw28jGXu2AIAAAAArMbEFgAAAABgNV73A9/p2LGjaztU+/GRI0c0F13CHEB8pKena77wwgtdx/7v//5Pc6hXd4XSunVr1/bSpUuLXxxOy+srfmLd2jt69GjNXluRzfNoRQ6uQoUKmkeOHKnZbPlr0qSJ6xrbWpFxsszMTM1m+3Go1/qY+4u2D+/YsUPz/PnzT/u55v5Vq1a5PotXAYXvVO3if8jKyopVOXHDHVsAAAAAgNWY2AIAAAAArEYrMnyncuXKns579dVXNe/cuTNa5QAohrS0NM0TJkxwHTPbo4rbimy2SSI+zHbgWDNbiYvWEap9um3bttEsKSE0a9Ys6P7Dhw9rpiXUfsOHD3dtDxs2THOo/xeb+83246L/PZityB9//LHm2rVrB/2sU7XMmnX16dMnaF34H/P3RfNn6feVwKOJO7YAAAAAAKsxsQUAAAAAWI1WZPhCSkqK5gcffDDkefn5+ZonT54c1ZoAFN8ll1yiuV69esW+/sCBA5rvvffeSJSE03jiiSeC7i/a8mvbysJeV08uzRo0aBB0f15enuacnJxYlYMoufzyy13bxV39+K233vL0PWb78HPPPafZy2rJIiK9evXSbP43uHDhQs2lvTW+fPnymhs3bnza87/++mvNpWEsc8cWAAAAAGA1JrYAAAAAAKvRioy4MduPp0+frjnUKo0iIrNnz9a8bdu26BQGoFiqVKmiuUuXLporVqzo6Xqz/XjgwIGaFy1aVOLacHqhVhX2q6Kt07bV7yeff/55vEtAlJjtx61atXId87JCfUlX1jVbhs225lWrVoX8brM12Vwh2WxRPtUKzaXBAw88oNn8WYRiPsK3b9++qNTkJ9yxBQAAAABYjYktAAAAAMBqtCIjbsaPH6/5lltuCXne2rVrNY8YMSKqNQEovmrVqmnu0aNHsa8vV66c5rPPPjsiNaH0MFdvDtWWbK6QbNvqztHUokWLoPu///772BaCiMvMzNRsrkQsIhIIBDSb7b/m6sfmSsQlZbYMt2nTRrO5WnLROs26zP21a9eOWF02OtXjesEsWbIkSpX4E3dsAQAAAABWY2ILAAAAALAaE1sAAAAAgNV4xhYxZb6mwcsy5SIiU6ZM0fzLL79EuiQAYRg0aJDmZ555RrP5XFRRoY5ddtllmr/77rsIVIfTKfrKnETHM7bFU7VqVc3mM/AiIsePH491OQiD+bqcU71Wxzxm/nkrWkK9Bkgk9KuAzHqHDh2q2Xz2Nha1x0PNmjVd240aNdIc6lnpjRs3ai76HHOi444tAAAAAMBqTGwBAAAAAFajFRlRV6lSJc2dOnXSfNZZZwU9/80333RtL126NDqFAQjbnj17NP/666+ak5OTQ16Tm5urecaMGZr37t0b4eoAeNGhQ4eg+1u2bKm5QYMGrmNmmyP861SPhYRqYTXbhGOh6PeZr3ds1aqVZrPe1q1bazZfHZSXl+f6LPPVRTarW7eua7tp06aaHcfRbLZuv/7665q3bdsWxer8hzu2AAAAAACrMbEFAAAAAFiNVmRERWpqqmaz5dBsLTHt379fc9HVOvPz8yNbHICwNGzYUHOXLl00p6Wlebq+evXqmrdv36754MGDEagOpYW5wrGIyKhRo057TWlbBdorr2MX9jFbU72uihxvvXr10jxv3jzNZvtxqNWSzbZc25mP6j366KOertm1a5fmmTNnRrwmW3DHFgAAAABgNSa2AAAAAACr0YqMqEhPT9fcp0+foOeY7cc333yzZlZcBPxp06ZNmsNpX/v00081L1myJCI1IXbMFuCPPvoopt9tthJ7aT0WERk9enSUqkkcP/30U9D93377bdAMe4SzKvLll1+uOdYrJIuI7NixQ7PZWhuqXnO/mW1nrlZ+/fXXe7rm5Zdf1my+taC04Y4tAAAAAMBqTGwBAAAAAFajFRkR0alTJ9f2lClTTnuN+fLs9957L+I1ASi5MWPGhH3tyJEjXduMc/8wW4lDtfYW3W9ut2/fPuhnRZLZ+uy1/dgU63ZpG5m/D99+++2azVXLT5w4EdOaEBnhrIo8bNgwzaEeI4sV88+RPXv21FwaVkXu1q1bsa8pye/ViYQ7tgAAAAAAqzGxBQAAAABYLeB4vHefSKuNIfJmz57t2r7llluCnrdv3z7N5qpvX3/9dXQKiwO/t8MwllEcGRkZms1VjUOtivz9999rzs3NdR3r2rVrZIuLMj+P5UiO45ycHM1m+69XZsvvihUrNJsrGRf9XHO7bdu2Eft+s0UaEPH3OBaJ3u/J5grH8+fPdx2rU6eOZvPnY9YyadIkzQsXLtQcj9WSMzMzNWdnZ2s26+3Vq5frGrPN3jbmowC1atXydE2ZMmWiVY5veBnL3LEFAAAAAFiNiS0AAAAAwGpMbAEAAAAAVkvY1/00a9bMtd2mTRvN06ZNK9ZnPf30067tJ598UvPRo0fDqC4xpKamam7QoIGna+6//37NifRcLZCozNesnOr1EX84cOCA5qysrKjVhcgxn0sN53lE87nYkr6ix4vRo0e7ts1neQH8znwW9uOPP3Ydq127tuZQr88xX/1jPr9a9LPWrl2r2curHk/FfC7Y/H7zmeBEfd1Po0aNNCcnJ2s+1a9r7NixUa3JRtyxBQAAAABYjYktAAAAAMBqCduKXLTd2GxFLm67wsMPP+zaNl9t880334RRXWIw279at24d8jyz5dhcMh6AP5ivEzBfpSBy8mMdwezatUvzX/7yF83mKwtgB7MtuWgrcTiv4imuUK8OMvebGcDp9enTx7Vdt25dza1atdJsvj7HbPM1W4HNNmYRd5vyxIkTg35WqFcKFf3zeHGvMffb/irDG264QXPVqlU9XXPw4MFolWMt7tgCAAAAAKzGxBYAAAAAYLWEakXu37+/5gsuuMB17FStD8U1fvx4zTfeeGOJPss2KSkpms32laL279+v2WxNLM2rSAN+dfXVV2vevXu365i5SmUoy5cv11yzZk3N5mqZsMOpWn5DrX7ctm3boPu9MtufaTMGos9sHzb/LLdgwQLNoVYfLroifqhjxd1f0s+yfVXkQ4cOaS4oKNBcpkwZ13nmsfz8/OgXZhnu2AIAAAAArMbEFgAAAABgtYRqRa5evbpm8+XGIpFtUbjqqqs0m6vD7dixI2Lf4VeVK1fWfKqVkM12xlWrVkW1JgAl8+uvv2ru0aNHsa8vV66c5kWLFkWkJvgPKxMDicH886qZzbbX4cOHa87MzNRc9DG0UCspF3f/qY699dZbms0/zxdd7dlmL7/8suamTZtqNh/nExGZMWOG5ldeeSX6hVmGO7YAAAAAAKsxsQUAAAAAWC3geOzRteHFxy1bttQ8Z84c17GGDRtqjmRb8uLFizWXhhWSzz33XM2nar3u16+f5qL/LhKd31fms2EsA37g57HMOAa88fM4FrFjLJuP3RVtRTZXzh82bJhmLysZT5o0yfVZn3zyiWbz39vChQvDqBqJxstY5o4tAAAAAMBqTGwBAAAAAFazvhW5efPmmmfPnq354osvdp1n1h/JtpTt27drHjRokOalS5dG7Dv8JFQr8qxZs1znDRw4UHPRF3AnOtqegMTg57HMOAa88fM4FmEsA17RigwAAAAASHhMbAEAAAAAVmNiCwAAAACw2hnxLiAc6enpmpcsWaK5Zs2anq4/cuRI0GxKTk7WnJqaGvKz6tatq9lcAv2DDz7Q/Ntvv3mqywa7d+/WXKZMmThWAgAAAAC/444tAAAAAMBqTGwBAAAAAFaz5nU/5mt9wmk/NuufMGGC5kceeSTo+XfccYfml156yWuZqk2bNprXrFlT7OthL14tACQGP49lxjHgjZ/HsQhjGfCK1/0AAAAAABIeE1sAAAAAgNWsWRV58ODBmr22H5seeughzWvXro1ITQAAAACA+OOOLQAAAADAakxsAQAAAABW83UrcteuXTXfcMMNJfqsZ599tqTlAAAAAAB8iDu2AAAAAACrMbEFAAAAAFjNV63IvXr1cm1nZ2ef9pr9+/drHjJkiOZ58+aVqJaffvpJ8549e1zHzj777NNen5TE3xkAAAAAQCww+wIAAAAAWI2JLQAAAADAakxsAQAAAABW89UzttOmTXNtO45z2mtWrlypuaTP1ZqWLFmi+Z133nEdGzBgwGmvLywsjFgtQCLp0aOH5sGDB2tu0KCB5mXLlml+6qmnXNfn5eVprlmzpubXXntN8+eff6551KhRmo8fPx5m1QAAAPAz7tgCAAAAAKzGxBYAAAAAYLW4tyI//vjjmlNTU+NYCYBoeeyxxzT36dNH88iRIzVv2rRJ8/333695y5Ytrs8y25fnz5+vecGCBZqffPJJzeZjAWYdAAAASBzcsQUAAAAAWI2JLQAAAADAanFpRb7vvvs0m62B5cqVC3nNrl27NHfp0kVz0TbFSKlSpYrmF1980XXsmmuu0VynTp2ofD9gM3OMiIhkZWVp7tChg+Yvvvgi6PV33XWX5lWrVrmOPf/885p37NiheebMmZorVaqkecyYMZrfe+8912etWLEi+C8AAAAAVuGOLQAAAADAakxsAQAAAABWi1kr8jnnnKP5yiuv1Hyq9mOTuXrq+vXrI1dYCDfddJPml156KerfBySSoUOHurbXrFmjOVT7sclxHM2zZs1yHdu/f7/madOmaa5cubJms1353nvv1Wz+f0RE5Oqrrz5tLQAAAPA/7tgCAAAAAKzGxBYAAAAAYLWYtSI3bNhQs9nmG2/mCs3NmjXT3KhRo2J/1oIFCzRHa7VmwK9at26tuXPnzq5jGRkZEfuexYsXaz5w4IDmQYMGaS5btqzmp556SnPRxwp69eql2Ry/AAAAsAt3bAEAAAAAVmNiCwAAAACwWsAxlx891YmBQIm+qE2bNppXrlxZ7Ot/+OEHzb/88kuxr09K+t8cvrCwUHPNmjU1p6amevos83qzLXLgwIGazZVbUbp4HFJxU9KxHIrZyl90jNevX1/zzz//HJXvD6VSpUqav/vuO9exnJwczb17945ZTbCDn8dytMYxkGj8PI5FGMuAV17GMndsAQAAAABWY2ILAAAAALAaE1sAAAAAgNVi9rqfffv2ac7NzdXcpEkTT9fXqFEjaPbKfIbBS4/2tm3bNK9bt851LD8/X3O/fv2KXQuQiBo0aKB59+7drmOxfq7WZI7XQ4cOuY6ZrwUCAACAvbhjCwAAAACwGhNbAAAAAIDVYtaKvHHjRs1/+9vfND/99NOxKuG0Ro8erXnt2rWaly1bFo9yAKts2bIl3iWc1okTJ1zbV1xxRZwqAQAAQCRxxxYAAAAAYDUmtgAAAAAAq8WsFdk0d+5czStXrtScnp4e8rySuvLKKzUXFhYGPeezzz7TXLRlEYB3KSkpru3k5GTNx44di3U5yvz/jYhIp06dNFeqVEmzuZIyAAAA/I87tgAAAAAAqzGxBQAAAABYLS6tyDt37gya16xZ4zovOzs7ZjUBKJkWLVporl27tutY1apVNe/atStmNZ2OWWf9+vU1b9iwIR7lAAAAIEzcsQUAAAAAWI2JLQAAAADAanFpRQaQ2AKBQLxLCGr9+vWubXP188OHD8e6HAAAAEQId2wBAAAAAFZjYgsAAAAAsBqtyAAi4vPPP9fsOI7rWNu2bTXPmzcvZjUVVadOHde22X78448/xrocAAAARAh3bAEAAAAAVmNiCwAAAACwGhNbAAAAAIDVeMYWQNQlJfnj79CqVq3q2t66davmQ4cOxbocAAAARIg//rQJAAAAAECYmNgCAAAAAKxGKzKAiMjNzdW8f/9+17GLLroo1uUEVatWLdf25s2b41QJAAAAIok7tgAAAAAAqzGxBQAAAABYjVZkABFx4sQJzUuWLHEdGzhwoOYpU6Zo3rNnT9TrqlSpkuYWLVq4jk2fPj3q3w8AAIDo444tAAAAAMBqTGwBAAAAAFajFRlAxG3YsMG13a9fP8133HGH5vHjx0e9lqysLM1nnOH+X96MGTOi/v0AAACIPu7YAgAAAACsxsQWAAAAAGC1gOM4jqcTA4Fo1wIkBI9DKm5iMZZr1qzp2s7JydFcv359zYMGDdL8yiuvROz7zdWP3333Xc0ffPCB67y+fftG7DuRePw8lvk9GfDGz+NYhLEMeOVlLHPHFgAAAABgNSa2AAAAAACr0YoMRBhtTydr1KiRZrMtuXr16pofeeQRzZMmTSr2d2RkZGj+17/+pXnTpk2ae/fu7bpm165dxf4elB5+Hsv8ngx44+dxLMJYBryiFRkAAAAAkPCY2AIAAAAArMbEFgAAAABgNZ6xBSKM53lOzXwVUHZ2tuYrrrhCs/lanlmzZrmur1q1quY//elPmjt06KB54cKFmgcPHqx537594ZaNUsjPYzne4xiwhZ/HsQhjGfCKZ2wBAAAAAAmPiS0AAAAAwGq0IgMRRtuTd2XLltU8Z84czZmZmZ6u//e//6157NixmpcvXx6B6lDa+Xks+2kcA37m53EswlgGvKIVGQAAAACQ8JjYAgAAAACsRisyEGG0PQGJwc9jmXEMeOPncSzCWAa8ohUZAAAAAJDwmNgCAAAAAKzGxBYAAAAAYDUmtgAAAAAAqzGxBQAAAABYjYktAAAAAMBqTGwBAAAAAFZjYgsAAAAAsBoTWwAAAACA1ZjYAgAAAACsxsQWAAAAAGA1JrYAAAAAAKsxsQUAAAAAWI2JLQAAAADAakxsAQAAAABWY2ILAAAAALAaE1sAAAAAgNWY2AIAAAAArBZwHMeJdxEAAAAAAISLO7YAAAAAAKsxsQUAAAAAWI2JLQAAAADAakxsAQAAAABWY2ILAAAAALAaE1sAAAAAgNWY2AIAAAAArMbEFgAAAABgNSa2AAAAAACr/T/3uUYZqLMe2AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "root_directory = './double_mnist'\n",
    "def display_images_from_folder(folder_path, num_images=4):\n",
    "    image_files = os.listdir(folder_path)\n",
    "    random_images = random.sample(image_files, num_images)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, image_file in enumerate(random_images):\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        image = imread(image_path)\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f\"Image {i + 1}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "train_folder = os.path.join(root_directory, 'train', '01') \n",
    "display_images_from_folder(train_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP On MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.5) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '01', '04', '05', '06', '08', '09', '11', '12', '13', '14', '15', '16', '18', '19', '20', '21', '23', '24', '26', '28', '29', '30', '31', '33', '35', '37', '38', '41', '42', '43', '44', '45', '50', '51', '53', '54', '56', '59', '60', '62', '63', '65', '69', '70', '72', '74', '75', '76', '77', '79', '81', '82', '84', '85', '87', '88', '89', '90', '91', '94', '95', '97', '98']\n",
      "Dataset CustomImageFolder\n",
      "    Number of datapoints: 64000\n",
      "    Root location: double_mnist/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Grayscale(num_output_channels=1)\n",
      "               Resize(size=(28, 28), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5,), std=(0.5,))\n",
      "           )\n",
      "['03', '07', '10', '22', '27', '34', '39', '40', '48', '52', '58', '61', '64', '71', '93', '99']\n",
      "tensor([82, 89, 29, 37, 20, 82, 79, 89, 63, 62, 21, 85, 69, 33, 41, 42,  8, 63,\n",
      "        94, 79, 90, 97, 18, 87, 50, 41, 65, 60,  5, 21, 11, 87, 88, 43, 79, 89,\n",
      "        18, 85, 97, 12, 11, 74, 87,  8, 24, 54, 28, 42, 53, 21, 43, 23, 94, 26,\n",
      "         8, 95, 29, 75, 30,  1, 26, 13, 23, 11])\n",
      "tensor(3)\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_data_path = 'double_mnist/train'\n",
    "val_data_path = 'double_mnist/val'\n",
    "test_data_path = 'double_mnist/test'\n",
    "class CustomImageFolder(ImageFolder):\n",
    "    def __init__(self, root, transform=None):\n",
    "        super(CustomImageFolder, self).__init__(root, transform=transform)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        folder_name = os.path.basename(os.path.dirname(path))\n",
    "        return sample, int(folder_name) \n",
    "    \n",
    "train_dataset = CustomImageFolder(train_data_path, transform=transform)\n",
    "val_dataset = CustomImageFolder(val_data_path, transform=transform)\n",
    "test_dataset = CustomImageFolder(test_data_path, transform=transform)\n",
    "print(train_dataset.classes)\n",
    "class_to_label = {str(i).zfill(2): i for i in range(100)}\n",
    "# Define batch sizes\n",
    "batch_size = 64\n",
    "print(train_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "num_classes = 10\n",
    "class_names = train_dataset.classes\n",
    "class_names = val_dataset.classes\n",
    "print(class_names)\n",
    "for images, labels in train_loader:\n",
    "    print(labels)\n",
    "    break\n",
    "for images, labels in val_loader:\n",
    "    plt.imshow(images[0].permute(1, 2, 0))\n",
    "    print(labels[0])\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CNN class similar to the provided one\n",
    "class MultiMNIST_CNN(nn.Module):\n",
    "    def __init__(self, input_channels , num_classes , dropout_size=0.25, strides=2, kernel_size=3, pool_size=2):\n",
    "        super(MultiMNIST_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=kernel_size, padding=int((kernel_size-1)/2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=pool_size, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=kernel_size, padding=int((kernel_size-1)/2))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=pool_size, stride=strides)\n",
    "        self.var1 = int((28-pool_size)/strides)+1\n",
    "        self.dim = int((self.var1-pool_size)/strides)+1\n",
    "        self.fc1 = nn.Linear(64 * self.dim * self.dim, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_size)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool1(self.relu1(self.conv1(x)))\n",
    "        x = self.maxpool2(self.relu2(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * self.dim * self.dim)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = MultiMNIST_CNN(input_channels=1,num_classes=num_classes)\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n",
    "    train_losses, val_losses = [], []\n",
    "    train_acc, val_acc = [] , []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            labels_group = labels//10\n",
    "            optimizer.zero_grad()\n",
    "            image = images.clone()\n",
    "            left_half = images[:, :, :, :images.shape[3] // 2]\n",
    "            image[:, :, :, (image.shape[3] // 2):] = left_half\n",
    "            outputs = model(image)\n",
    "            loss = criterion(outputs, labels_group)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            for k in range(labels.size(0)):\n",
    "                if(labels_group[k]==predicted[k]):\n",
    "                    correct_train = correct_train+1\n",
    "\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        train_acc.append(train_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                labels_group = labels // 10\n",
    "                image = images.clone()\n",
    "                left_half = images[:, :, :, :images.shape[3] // 2]\n",
    "                image[:, :, :, (image.shape[3] // 2):] = left_half\n",
    "                outputs = model(image)\n",
    "                loss = criterion(outputs, labels_group)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels_group).sum().item()\n",
    "                count = count+1\n",
    "\n",
    "            val_accuracy = 100 * correct_val / total_val\n",
    "            val_losses.append(val_loss / len(val_loader))\n",
    "            val_acc.append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracy:.2f}%, '\n",
    "              f'Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
    "\n",
    "    return train_losses, val_losses, train_acc, val_acc\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            labels_group = labels // 10\n",
    "            image = images.clone()\n",
    "            left_half = images[:, :, :, :images.shape[3] // 2]\n",
    "            image[:, :, :, (image.shape[3] // 2):] = left_half\n",
    "            outputs = model(image)\n",
    "            loss = criterion(outputs, labels_group)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels_group).sum().item()\n",
    "            count = count+1\n",
    "        test_accuracy = 100 * correct_test / total_test\n",
    "        average_test_loss = (test_loss / len(test_loader))\n",
    "\n",
    "    print(f'Test Loss: {average_test_loss:.4f}, Test Acc: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.6324, Train Acc: 79.24%, Val Loss: 0.1849, Val Acc: 94.56%\n",
      "Epoch 2/5, Train Loss: 0.1911, Train Acc: 94.17%, Val Loss: 0.1305, Val Acc: 96.01%\n",
      "Epoch 3/5, Train Loss: 0.1343, Train Acc: 95.89%, Val Loss: 0.1013, Val Acc: 96.78%\n",
      "Epoch 4/5, Train Loss: 0.1089, Train Acc: 96.62%, Val Loss: 0.0868, Val Acc: 97.27%\n",
      "Epoch 5/5, Train Loss: 0.0888, Train Acc: 97.20%, Val Loss: 0.0881, Val Acc: 97.39%\n",
      "Test Loss: 0.0942, Test Acc: 97.16%\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, train_acc, val_acc = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "test_model(model,test_loader=test_loader,criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=3):\n",
    "    train_losses, val_losses = [], []\n",
    "    train_acc, val_acc = [] , []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            labels_group = labels % 10\n",
    "            optimizer.zero_grad()\n",
    "            image = images.clone()\n",
    "            right_half = images[:, :, :, images.shape[3] // 2:]\n",
    "            image[:, :, :, :(image.shape[3] // 2)] = right_half\n",
    "            outputs = model(image)\n",
    "            loss = criterion(outputs, labels_group)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            for k in range(labels.size(0)):\n",
    "                if(labels_group[k]==predicted[k]):\n",
    "                    correct_train = correct_train+1\n",
    "\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        train_acc.append(train_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                labels_group = labels % 10\n",
    "                image = images.clone()\n",
    "                right_half = images[:, :, :, images.shape[3] // 2:]\n",
    "                image[:, :, :, :(image.shape[3] // 2)] = right_half\n",
    "                outputs = model(image)\n",
    "                loss = criterion(outputs, labels_group)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels_group).sum().item()\n",
    "                count = count+1\n",
    "\n",
    "            val_accuracy = 100 * correct_val / total_val\n",
    "            val_losses.append(val_loss / len(val_loader))\n",
    "            val_acc.append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracy:.2f}%, '\n",
    "              f'Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
    "\n",
    "    return train_losses, val_losses, train_acc, val_acc\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            labels_group = labels % 10\n",
    "            image = images.clone()\n",
    "            right_half = images[:, :, :, images.shape[3] // 2:]\n",
    "            image[:, :, :, :(image.shape[3] // 2)] = right_half\n",
    "            outputs = model(image)\n",
    "            loss = criterion(outputs, labels_group)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels_group).sum().item()\n",
    "            count = count+1\n",
    "        test_accuracy = 100 * correct_test / total_test\n",
    "        average_test_loss = (test_loss / len(test_loader))\n",
    "\n",
    "    print(f'Test Loss: {average_test_loss:.4f}, Test Acc: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 0.0722, Train Acc: 97.70%, Val Loss: 0.0788, Val Acc: 97.54%\n",
      "Epoch 2/3, Train Loss: 0.0607, Train Acc: 98.04%, Val Loss: 0.0635, Val Acc: 98.18%\n",
      "Epoch 3/3, Train Loss: 0.0524, Train Acc: 98.30%, Val Loss: 0.0762, Val Acc: 97.80%\n",
      "Test Loss: 0.0763, Test Acc: 97.75%\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, train_acc, val_acc = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "test_model(model,test_loader=test_loader,criterion=criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permuted MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the dataset, you can use np.load\n",
    "import numpy as np\n",
    "data = np.load(\"permuted_mnist.npz\")\n",
    "permuted_x_train = data[\"train_images\"]\n",
    "y_train = data[\"train_labels\"]\n",
    "permuted_x_test = data[\"test_images\"]\n",
    "y_test = data[\"test_labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN On Permuted MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
